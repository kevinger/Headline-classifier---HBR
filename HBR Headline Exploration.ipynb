{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBR Headline exploration and prediction\n",
    "\n",
    "Donec at dapibus magna. Sed sed tristique purus. Vivamus in pretium metus. Pellentesque dolor metus, placerat placerat tristique vitae, vehicula ut risus. Quisque metus ligula, interdum eu tempus vel, porta at arcu. Curabitur sit amet lacus elit. Praesent gravida consequat nibh ultricies ornare. Nullam consectetur dapibus scelerisque. Praesent venenatis odio at neque blandit venenatis. Quisque id commodo leo. Sed cursus leo ut dui semper tincidunt. Phasellus bibendum et elit id posuere.\n",
    "\n",
    "Aenean tempus, sem ac tincidunt lobortis, diam ligula molestie dolor, vitae aliquam turpis purus at erat. Duis rhoncus odio ipsum, nec placerat ex suscipit non. Nullam egestas vulputate neque, at varius arcu dignissim at. Maecenas eget faucibus nisi. Vivamus vitae posuere justo. Quisque sit amet viverra mi. Etiam facilisis in diam euismod fermentum. Ut ultricies finibus elit, auctor elementum quam dictum a. Proin semper massa erat, a tempor arcu lobortis nec. Suspendisse sed efficitur ante. Vestibulum tortor dolor, feugiat nec fermentum nec, lobortis ut ligula. Duis nec facilisis tellus. Sed fermentum ante felis, ornare dignissim tellus auctor eu. Integer mollis feugiat nibh ut luctus. Donec ac vestibulum erat. Ut efficitur purus aliquet, pharetra arcu vel, molestie sapien.\n",
    "\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore our headlines\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut nulla neque, hendrerit ac rutrum ac, accumsan vitae elit. Duis nisl odio, ornare sit amet scelerisque ac, cursus quis arcu. In vel tristique dui, vitae pretium turpis. Integer scelerisque suscipit dolor non vehicula. Praesent non nisl odio. Maecenas vulputate lectus libero, quis placerat mi viverra eu. Nulla facilisi. Sed dictum aliquam mattis. Nunc non tellus tincidunt, viverra mauris et, iaculis eros. Nam felis ex, blandit a maximus quis, auctor ac ante. Vestibulum ac consectetur est, pulvinar iaculis purus. In malesuada eros sed eros suscipit faucibus. Vivamus quis tristique ex, id ultrices mi. Maecenas congue auctor convallis. Vivamus vehicula purus sit amet magna ullamcorper efficitur.\n",
    "\n",
    "Headlines data consists of 5000 items with two features: \n",
    "\n",
    "- **Page Title**: text for headline for article\n",
    "- **Topic**: text describing editorially assigned topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load headlines and see a few\n",
    "data = pd.read_csv(\"headlines.csv\")\n",
    "print \"Loaded {:,} headlines\".format(len(data.index))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding processing functions\n",
    "\n",
    "Further on we'll be applying a [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) approach, so filtering out stop words ahead of time will help make the titles more usable. Here we're adding in the two processing functions, `remove_stop_words` and `lowercase_tokens` for those next steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some text functions\n",
    "\n",
    "#Takes a list of words, remove common ones\n",
    "def remove_stop_words(tokens):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in tokens if w.lower() not in stopwords]\n",
    "    return content\n",
    "\n",
    "#Takes a list of words, lowercases them\n",
    "def lowercase_tokens(tokens):\n",
    "    list_of_words = []\n",
    "    for a in tokens:\n",
    "        a = a.lower()\n",
    "        list_of_words.append(a)\n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the headlines data pulled in from the spreadsheet above, let's clean up the text with those functions,  and take a look at what we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the most common words in headlines\n",
    "\n",
    "#Concatenate headlines into one string\n",
    "text = data['Page Title'].str.cat(sep=\" \")\n",
    "\n",
    "#Encode as utf-8 \n",
    "text = text.decode(\"utf-8\")\n",
    "\n",
    "#Create tokenizer to remove punctuation and numbers\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#Tokenize into words, lowercase, remove stop words\n",
    "tokens = tokenizer.tokenize(text) \n",
    "tokens = lowercase_tokens(tokens)\n",
    "tokens = remove_stop_words(tokens)\n",
    "\n",
    "#Print common words\n",
    "fdist = FreqDist(tokens)\n",
    "print \"The most common words in our headlines are:\"\n",
    "# print fdist.most_common(50)\n",
    "\n",
    "plt.figure(figsize=(15, 4))  # the size you want\n",
    "fdist.plot(35, cumulative=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional cleanup, further processing the text  \n",
    "The exploration showed that lots of headlines have \"HBR\" in them. We need to fix that, look for similar issues, and then get rid of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset headlines that have \"HBR\" and other issues in them to see how they look\n",
    "\n",
    "hbr = data[data['Page Title'].str.contains(\"HBR\")]\n",
    "print hbr.shape\n",
    "\n",
    "hbr2 = data[data['Page Title'].str.contains(\"Harvard Business Review\")]\n",
    "print hbr2.shape\n",
    "\n",
    "for a in hbr['Page Title'][0:5]:\n",
    "    print a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix headline formatting and remove duplicates\n",
    "data['Clean Title'] = data['Page Title'].str.replace(' - HBR', '')\n",
    "\n",
    "#Check for remaining 'HBR' mentions\n",
    "hbr = data[data['Clean Title'].str.contains(\"HBR\")]\n",
    "print hbr.shape\n",
    "print hbr.head()\n",
    "\n",
    "#We've cut 400+ headlines down to 15. Now let's just remove \"HBR\" from those remaining heds\n",
    "data['Clean Title'] = data['Clean Title'].str.replace('HBR', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate headlines\n",
    "data = data.drop_duplicates(subset=\"Clean Title\")\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the cleaned headlines to a new file\n",
    "We've got ~4500 cleaned up headlines left. Time to save them to a new csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Clean Title'].to_csv(\"clean_headlines.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load social media data\n",
    "To add to the dataset, add social media posts from Twitter & Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social = pd.read_csv(\"all_tweets_fb_2013-nov18.csv\")\n",
    "print social.shape\n",
    "\n",
    "for column in social.columns:\n",
    "    print column\n",
    "\n",
    "\n",
    "display(social['Created By'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limit the dataset to tweets and Facebook posts by HBR editors, as opposed to marketing, etc.\n",
    "\n",
    "editors = ['nicole.torres@harvardbusiness.org','alexandra.kephart@hbr.org', 'Ramsey.Khabbaz@harvardbusiness.org',\n",
    "     'paige.cohen@hbr.org','nicole.blank@hbr.org','ggavett@hbr.org','etruxler@hbr.org','awieckowski@hbr.org',\n",
    "     'duygu.mullin@hbr.org','walter.frick@harvardbusiness.org']\n",
    "social = social[social['Created By'].isin(editors)]\n",
    "social.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(social['Message'].head(5))\n",
    "social['Message'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining text\n",
    "Let's combine the headlines and social media data for ...(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine text fields for HBR headlines and social media\n",
    "\n",
    "tk = data['Clean Title']\n",
    "tk2 = social['Message']\n",
    "\n",
    "frames = [tk,tk2]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "df = result.to_frame(name='Clean Title')\n",
    "print df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More cleanup! \n",
    "Additional cleanup because ... (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove mentions of HBR\n",
    "df['Clean Title'] = df['Clean Title'].str.replace(' - HBR', '')\n",
    "df['Clean Title'] = df['Clean Title'].str.replace('HBR', '')\n",
    "\n",
    "#Remove mentions of research because the algorithm is being used to evaluate research\n",
    "df['Clean Title'] = df['Clean Title'].str.replace('Research', '')\n",
    "df['Clean Title'] = df['Clean Title'].str.replace('research', '')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data! \n",
    "Save the combined dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Clean Title'].to_csv(\"clean_headlines.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## From sheet 2: Non-HBR Headline Aggregator Exploration\n",
    "Sed vitae mi urna. Ut pharetra varius nisl, ut efficitur elit venenatis pharetra. Nulla molestie vulputate justo vel pretium. Aenean a risus justo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Mauris vel purus sit amet ipsum rhoncus venenatis. Donec eget nisl venenatis, vestibulum sapien ultricies, faucibus risus. Phasellus hendrerit orci eget leo posuere ullamcorper sit amet ut justo. Maecenas at consectetur dolor. Aliquam dignissim molestie nulla, id consequat turpis consequat tincidunt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load headline dataset and take a look\n",
    "# https://www.kaggle.com/uciml/news-aggregator-dataset\n",
    "# https://archive.ics.uci.edu/ml/datasets/News+Aggregator\n",
    "\n",
    "df = pd.read_csv(\"uci-news-aggregator.csv\")\n",
    "print df.shape\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter News Data\n",
    "Let's winnow down this data set to only include 'Business' stories, and then only their headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Randomly select 15,000 headlines (to match size of HBR dataset)\n",
    "random_df = df.sample(n=15000)\n",
    "\n",
    "#Save headlines from random set to csv\n",
    "random_df['TITLE'].to_csv(\"other_headlines.csv\",index=False,header=True)\n",
    "\n",
    "#Do the same just for business headlines\n",
    "biz = df.loc[df['CATEGORY'] == 'b']\n",
    "random_biz = biz.sample(n=15000)\n",
    "\n",
    "#Save random business headlines to cvs\n",
    "random_biz['TITLE'].to_csv(\"other_biz_headlines.csv\",index=False,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# From sheet 3\n",
    "Nulla facilisi. Praesent a sapien sit amet diam finibus rutrum a at nunc. Pellentesque ut dui dictum, tempus orci non, luctus ipsum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin aliquam gravida lacinia. Phasellus vel nibh eget dui sagittis fermentum non ut ipsum. Sed posuere mi eget quam vehicula tincidunt. Sed auctor, arcu at blandit molestie, lacus purus scelerisque lacus, sit amet sagittis dui nunc at sapien. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine clean HBR headlines with sampling of aggregator headlines and add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbr = pd.read_csv(\"clean_headlines.csv\")\n",
    "hbr.columns = ['Headline']\n",
    "hbr['HBR'] = 'Yes'\n",
    "print hbr.shape\n",
    "hbr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others = pd.read_csv(\"other_headlines.csv\")\n",
    "others.columns = ['Headline']\n",
    "others['HBR'] = 'No'\n",
    "print others.shape\n",
    "others.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([hbr, others])\n",
    "print combined.shape\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data!\n",
    "Save the combined dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined.to_csv(\"combined_headlines.csv\",index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for business headlines only\n",
    "\n",
    "Insert details here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz = pd.read_csv(\"other_biz_headlines.csv\")\n",
    "biz.columns = ['Headline']\n",
    "biz['HBR'] = 'No'\n",
    "print biz.shape\n",
    "biz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_biz = pd.concat([hbr, biz])\n",
    "print combined_biz.shape\n",
    "combined_biz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_biz.to_csv(\"combined_biz_headlines.csv\",index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review this next block\n",
    "\n",
    "I'm not sure what this is for...some structure might help? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load headline dataset and take a look\n",
    "# https://www.kaggle.com/uciml/news-aggregator-dataset\n",
    "# https://archive.ics.uci.edu/ml/datasets/News+Aggregator\n",
    "\n",
    "df = pd.read_csv(\"uci-news-aggregator.csv\")\n",
    "print df.shape\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "#Randomly select 15,000 headlines (to match size of HBR dataset)\n",
    "random_df = df.sample(n=15000)\n",
    "\n",
    "#Save headlines from random set to csv\n",
    "random_df['TITLE'].to_csv(\"other_headlines.csv\",index=False,header=True)\n",
    "\n",
    "#Do the same just for business headlines\n",
    "biz = df.loc[df['CATEGORY'] == 'b']\n",
    "random_biz = biz.sample(n=15000)\n",
    "\n",
    "#Save random business headlines to cvs\n",
    "random_biz['TITLE'].to_csv(\"other_biz_headlines.csv\",index=False,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (from sheet 4) Train Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and check data\n",
    "heds = pd.read_csv(\"combined_headlines.csv\")\n",
    "print heds.shape\n",
    "print heds.describe()\n",
    "heds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train/test\n",
    "train = heds.sample(frac=0.7, random_state=1)\n",
    "test = heds.loc[~heds.index.isin(train.index)]\n",
    "\n",
    "print \"Train shape:\"\n",
    "print train.shape\n",
    "print \"Test shape\"\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize data\n",
    "\n",
    "#Initialize countvectorizer and fit to headlines\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             stop_words = 'english',\n",
    "                             ngram_range=(1,2),\n",
    "                             max_features=1000)\n",
    "\n",
    "train_counts = vectorizer.fit_transform(train['Headline'])\n",
    "test_counts = vectorizer.transform(test['Headline'])\n",
    "\n",
    "#Initialize tfidf transformer and fit to counts\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "print train_tfidf.shape\n",
    "\n",
    "test_tfidf = tfidf_transformer.transform(test_counts)\n",
    "print test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and evaluate linear model\n",
    "\n",
    "#Train logistic regression model on training set\n",
    "logit = linear_model.LogisticRegression(penalty='l1',C=1)\n",
    "logit = logit.fit(train_tfidf,train['HBR'])\n",
    "\n",
    "#Evaluate training performance\n",
    "logit_train_results = logit.predict(train_tfidf)\n",
    "train_output = pd.DataFrame(data={\"Headline\":train['Headline'],\n",
    "                                 \"HBR\":train[\"HBR\"],\n",
    "                                 \"Prediction\":logit_train_results})\n",
    "print train_output.head(30)\n",
    "\n",
    "print \"\\nThe accuracy of the model on the training set is:\"\n",
    "print accuracy_score(train_output['HBR'],train_output['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions against test data and assess performance\n",
    "logit_test_results = logit.predict(test_tfidf)\n",
    "test_output = pd.DataFrame(data={\"Headline\":test['Headline'],\n",
    "                                 \"HBR\":test[\"HBR\"],\n",
    "                                 \"Prediction\":logit_test_results})\n",
    "print test_output.head(30)\n",
    "\n",
    "print \"\\nThe accuracy of the model on the test set is:\"\n",
    "print accuracy_score(test_output['HBR'],test_output['Prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('hbr_logit.pkl','wb') as f:\n",
    "    pickle.dump(logit,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn text into dataframe to test\n",
    "text = \"How to manage a company\"   \n",
    "text = [text]\n",
    "df = pd.DataFrame(text,columns =['text'])    \n",
    "    \n",
    "#Vectorize text\n",
    "tk_features = vectorizer.transform(df['text'])\n",
    "tk_features = tfidf_transformer.transform(tk_features)\n",
    "    \n",
    "#Return prediction\n",
    "result = logit.predict(tk_features)\n",
    "print result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo the same process, but using a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline makes it easier to vectorize new data later on -- to save the vectorizer along with the model\n",
    "\n",
    "vect = CountVectorizer(analyzer='word',\n",
    "                             stop_words = 'english',\n",
    "                             ngram_range=(1,2),\n",
    "                             max_features=1000)\n",
    "\n",
    "#Define pipeline with countvectorizer, tfidftransformer, and logit model\n",
    "test_pipe = Pipeline([\n",
    "     ('vectorizer', vect),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('logit', linear_model.LogisticRegression(penalty='l1',C=1))\n",
    " ])\n",
    "\n",
    "#Fit logistic regression model with training data\n",
    "test_pipe.fit(train['Headline'], train[\"HBR\"]) \n",
    "\n",
    "#Make predictions against test data and assess performance\n",
    "predictions = test_pipe.predict(test['Headline'])\n",
    "test_output = pd.DataFrame(data={\"Headline\":test['Headline'],\n",
    "                                 \"HBR\":test[\"HBR\"],\n",
    "                                 \"Prediction\":predictions})\n",
    "print test_output.head(30)\n",
    "\n",
    "print \"\\nThe accuracy of the model on the test set is:\"\n",
    "print accuracy_score(test_output['HBR'],test_output['Prediction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the features with the lowest and highest coefficients\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = logit.coef_[0].argsort()\n",
    "print('Smallest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:100]]))\n",
    "print('Largest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:-101:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test new text (defined above) with fitted model\n",
    "\n",
    "print type(text)\n",
    "print type(test['Headline'])\n",
    "\n",
    "print test_pipe.predict(pd.Series(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save pipeline for later\n",
    "\n",
    "with open('hbr_pipeline.pkl','wb') as f:\n",
    "    pickle.dump(test_pipe,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat pipeline building and training with business heds only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and check data\n",
    "heds = pd.read_csv(\"combined_biz_headlines.csv\")\n",
    "print heds.shape\n",
    "print heds.describe()\n",
    "heds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train/test\n",
    "train = heds.sample(frac=0.7, random_state=1)\n",
    "test = heds.loc[~heds.index.isin(train.index)]\n",
    "\n",
    "print \"Train shape:\"\n",
    "print train.shape\n",
    "print \"Test shape\"\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline makes it easier to vectorize new data later on -- to save the vectorizer along with the model\n",
    "\n",
    "vect = CountVectorizer(analyzer='word',\n",
    "                             stop_words = 'english',\n",
    "                             ngram_range=(1,2),\n",
    "                             max_features=1000)\n",
    "\n",
    "#Define pipeline with countvectorizer, tfidftransformer, and logit model\n",
    "test_pipe = Pipeline([\n",
    "     ('vectorizer', vect),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('logit', linear_model.LogisticRegression(penalty='l1',C=1))\n",
    " ])\n",
    "\n",
    "#Fit logistic regression model with training data\n",
    "test_pipe.fit(train['Headline'], train[\"HBR\"]) \n",
    "\n",
    "#Make predictions against test data and assess performance\n",
    "predictions = test_pipe.predict(test['Headline'])\n",
    "test_output = pd.DataFrame(data={\"Headline\":test['Headline'],\n",
    "                                 \"HBR\":test[\"HBR\"],\n",
    "                                 \"Prediction\":predictions})\n",
    "print test_output.head(30)\n",
    "\n",
    "print \"\\nThe accuracy of the model on the test set is:\"\n",
    "print accuracy_score(test_output['HBR'],test_output['Prediction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the features with the lowest and highest coefficients\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = logit.coef_[0].argsort()\n",
    "print('Smallest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:100]]))\n",
    "print('Largest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:-101:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test new text (defined above) with fitted model\n",
    "\n",
    "print type(text)\n",
    "print type(test['Headline'])\n",
    "\n",
    "print test_pipe.predict(pd.Series(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save pipeline for later\n",
    "\n",
    "with open('hbr_biz_pipeline.pkl','wb') as f:\n",
    "    pickle.dump(test_pipe,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (from sheet 5) Apply Model to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to take text and score it using the model\n",
    "\n",
    "def predictor(text):\n",
    "    #Takes a string, returns prediction of whether it's HBR-relevant\n",
    "    pipeline = joblib.load('hbr_pipeline.pkl')\n",
    "    pipeline2 = joblib.load('hbr_biz_pipeline.pkl')\n",
    "    return pipeline.predict(pd.Series([text]))[0], pipeline2.predict(pd.Series([text]))[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try any text and see what the model says\n",
    "tk = 'Tesla is having major supply chain problems'\n",
    "predictor(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorize results form an RSS feed as HBR-relevant or not\n",
    "\n",
    "import feedparser\n",
    "feed = 'http://www.nber.org/rss/new.xml'\n",
    "#feed = 'http://feeds.hbr.org/harvardbusiness'\n",
    "#feed = 'https://news.ycombinator.com/rss'\n",
    "#feed = 'https://theconversation.com/us/articles.atom'\n",
    "feed = feedparser.parse(feed)\n",
    "for a in feed.entries:\n",
    "    title = a.title\n",
    "    print str(predictor(title)) + \": \" + title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
